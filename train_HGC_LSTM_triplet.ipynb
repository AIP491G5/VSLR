{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import project modules from new structure\n",
    "from configs.config import Config\n",
    "from src.utils.data_utils import (\n",
    "    load_triplet_csv, \n",
    "    TripletSignLanguageDataset, \n",
    "    create_triplet_data_loaders,\n",
    ")\n",
    "from src.utils.model_utils import (\n",
    "    create_adjacency_matrix,\n",
    "    create_model,\n",
    "    create_triplet_model,\n",
    "    load_model\n",
    ")\n",
    "from src.models.model import (\n",
    "    HGC_LSTM\n",
    ")\n",
    "from src.models.triplet import (\n",
    "    TripletNet\n",
    ")\n",
    "from src.utils.train_utils import (\n",
    "    train_model_triplet\n",
    ")\n",
    "from src.utils.visualization_utils import (\n",
    "    visualize_training_process,\n",
    "    analyze_model_performance\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"üì¶ Project structure reorganized with modular imports\")\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîß CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "print()\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"üîß Configuration loaded from configs/config.py\")\n",
    "print(f\"üìä Data from: {config.data.input_kp_path}\")\n",
    "print(f\"üíæ Models save to: {config.model.checkpoint_dir}\")\n",
    "print(f\"üìà Plots save to: {config.output.plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data utilities\n",
    "from src.utils.data_utils import load_labels_from_csv\n",
    "\n",
    "# Load labels\n",
    "triplet_data = load_triplet_csv(None, config)\n",
    "triplet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with stratified split and augmentation\n",
    "print(\"[INFO] Creating datasets...\")\n",
    "keypoints_dir = config.data.keypoints_output_dir\n",
    "\n",
    "# Set random seed for reproducible splits\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get parameters from config\n",
    "use_strategy = config.data.use_strategy\n",
    "\n",
    "\n",
    "# Training uses augmentation, validation does not (for fair evaluation)\n",
    "train_augmentations = getattr(config.data, 'augmentations', [])\n",
    "val_augmentations = [] \n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"   Split strategy: {'Stratified' if use_strategy else 'Random'}\")\n",
    "print(f\"   Train augmentations: {train_augmentations if train_augmentations else 'None'}\")\n",
    "print(f\"   Val augmentations: {val_augmentations if val_augmentations else 'None (for fair evaluation)'}\")\n",
    "if 'translation' in train_augmentations:\n",
    "    print(f\"   Translation range: ¬±{config.data.translation_range}\")\n",
    "if 'scaling' in train_augmentations:\n",
    "    print(f\"   Scale range: ¬±{config.data.scale_range}\")\n",
    "\n",
    "train_dataset = TripletSignLanguageDataset(\n",
    "    keypoints_dir, triplet_data, config,\n",
    "    split_type='train', \n",
    "    augmentations=train_augmentations,\n",
    "    use_strategy=use_strategy\n",
    ")\n",
    "\n",
    "val_dataset = TripletSignLanguageDataset(\n",
    "    keypoints_dir, triplet_data, config,\n",
    "    split_type='val', \n",
    "    augmentations=val_augmentations,\n",
    "    use_strategy=use_strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac722204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[INFO] Creating data loaders...\")\n",
    "train_loader, val_loader = create_triplet_data_loaders(train_dataset, val_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ad2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model utilities\n",
    "from src.utils.model_utils import create_adjacency_matrix\n",
    "\n",
    "A = create_adjacency_matrix(config)\n",
    "print(f\"[INFO] Adjacency matrix shape: {A.shape}\")\n",
    "print(f\"[INFO] Number of vertices: {config.hgc_lstm.num_vertices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31471",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_triplet_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d719eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Starting training...\")\n",
    "print(f\"[INFO] Training configuration:\")\n",
    "print(f\"  - Epochs: {config.training.num_epochs}\")\n",
    "print(f\"  - Batch size: {config.training.batch_size}\")\n",
    "print(f\"  - Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  - Optimizer: {config.training.optimizer}\")\n",
    "print(f\"  - Scheduler: {config.training.scheduler}\")\n",
    "print(f\"  - Early stopping patience: {config.training.early_stopping_patience}\")\n",
    "history = train_model_triplet(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc62d2",
   "metadata": {},
   "source": [
    "# Training Process Visualization\n",
    "\n",
    "This section provides essential visualization of the training process including:\n",
    "- **Loss curves**: Training and validation loss over epochs\n",
    "- **Accuracy curves**: Training and validation accuracy over epochs  \n",
    "- **Confusion matrix**: Model performance analysis on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae34800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization utilities\n",
    "from src.utils.visualization_utils import visualize_training_triplet_process\n",
    "\n",
    "# Visualize training process\n",
    "print(\"[INFO] Generating training process visualization...\")\n",
    "training_fig = visualize_training_triplet_process(history, config, save_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.model_utils import load_model_triplet\n",
    "import os\n",
    "model_save_path = os.path.join(config.model.checkpoint_dir, config.model.save_name)\n",
    "model = load_model_triplet(model_save_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f50b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.inference import extract_embedding_from_video\n",
    "import glob\n",
    "from src.utils.detector import MediaPipeProcessor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "test_dir = \"data/datatest\"  # Replace with your video path\n",
    "videos = glob.glob(os.path.join(test_dir, \"*.mp4\"))\n",
    "count = 0\n",
    "config = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = MediaPipeProcessor(config)\n",
    "\n",
    "# Load keypoints to embedding and add to database\n",
    "X_db, y_db = [], []\n",
    "keypoints_dir = \"dataset/Keypoints\"\n",
    "keypoints = glob.glob(os.path.join(keypoints_dir, \"*.npy\"))\n",
    "\n",
    "# Group keypoints by class number\n",
    "class_samples = defaultdict(list)\n",
    "for keypoint_path in keypoints:\n",
    "    filename = os.path.basename(keypoint_path)\n",
    "    number = int(filename.split('_')[0].split('.')[0])\n",
    "    class_samples[number].append(keypoint_path)\n",
    "\n",
    "# Take only 2 samples per class\n",
    "print(\"[INFO] Building database with 2 samples per class...\")\n",
    "for class_num, samples in class_samples.items():\n",
    "    # Take first 2 samples for each class\n",
    "    selected_samples = samples[:5]\n",
    "    \n",
    "    for keypoint_path in selected_samples:\n",
    "        embedding = model(torch.from_numpy(np.load(keypoint_path)).unsqueeze(0).float().to(device))\n",
    "        embedding = embedding.cpu().detach().numpy()\n",
    "        X_db.append(embedding)\n",
    "        y_db.append(class_num)\n",
    "    \n",
    "    print(f\"Class {class_num}: {len(selected_samples)} samples added\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_db = torch.cat([torch.from_numpy(arr) for arr in X_db]).numpy()\n",
    "y_db = np.array(y_db)\n",
    "\n",
    "print(f\"[INFO] Database built with {len(X_db)} total samples\")\n",
    "print(f\"[INFO] Number of classes: {len(np.unique(y_db))}\")\n",
    "print(f\"[INFO] Samples per class: {len(X_db) / len(np.unique(y_db)):.1f}\")\n",
    "\n",
    "# Train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_db, y_db)\n",
    "\n",
    "processor = MediaPipeProcessor(config)\n",
    "for video_path in videos:\n",
    "    filename = os.path.basename(video_path)\n",
    "    number = int(filename.split('_')[1].split('.')[0])\n",
    "    embedding = extract_embedding_from_video(model, processor, video_path, config, device)\n",
    "    pred_label = knn.predict(embedding)\n",
    "    print(f\"{number}: {pred_label}\")\n",
    "    if number == pred_label:\n",
    "        count += 1\n",
    "print(f\"{count}/{len(videos)} videos predicted correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional data from data/data_tl_test folder\n",
    "print(\"\\n=== LOADING ADDITIONAL TEST DATA ===\")\n",
    "\n",
    "# Load videos from data_tl_test folder\n",
    "test_tl_dir = \"data/data_tl_test\"\n",
    "test_tl_videos = glob.glob(os.path.join(test_tl_dir, \"*.mp4\"))\n",
    "\n",
    "print(f\"[INFO] Found {len(test_tl_videos)} videos in {test_tl_dir}\")\n",
    "\n",
    "# Group videos by label (gloss)\n",
    "test_tl_groups = defaultdict(list)\n",
    "for video_path in test_tl_videos:\n",
    "    filename = os.path.basename(video_path)\n",
    "    # Extract label from filename format: label_..._No...mp4\n",
    "    label = filename.split('_')[0]\n",
    "    test_tl_groups[label].append(video_path)\n",
    "\n",
    "print(f\"[INFO] Found {len(test_tl_groups)} different glosses:\")\n",
    "for label, videos in test_tl_groups.items():\n",
    "    print(f\"  - {label}: {len(videos)} videos\")\n",
    "\n",
    "# Convert X_db back to list for appending new data\n",
    "if isinstance(X_db, np.ndarray):\n",
    "    X_db = X_db.tolist()\n",
    "if isinstance(y_db, np.ndarray):\n",
    "    y_db = y_db.tolist()\n",
    "\n",
    "print(f\"[INFO] Current database before adding data_tl_test:\")\n",
    "print(f\"  - Total samples: {len(X_db)}\")\n",
    "print(f\"  - Unique labels: {len(set(y_db))}\")\n",
    "\n",
    "# Add 4 videos per gloss to database, keep 1 for testing\n",
    "print(\"\\n[INFO] Adding 4 videos per gloss to database, keeping 1 for testing...\")\n",
    "\n",
    "test_tl_videos_for_test = []\n",
    "test_tl_labels_for_test = []\n",
    "\n",
    "for label, videos in test_tl_groups.items():\n",
    "    if len(videos) >= 5:\n",
    "        # Take first 4 videos for database\n",
    "        db_videos = videos[:4]\n",
    "        # Keep 5th video for testing\n",
    "        test_video = videos[4]\n",
    "        \n",
    "        print(f\"\\n[INFO] Processing gloss '{label}':\")\n",
    "        print(f\"  - Adding {len(db_videos)} videos to database\")\n",
    "        print(f\"  - Keeping 1 video for testing: {os.path.basename(test_video)}\")\n",
    "        \n",
    "        # Add to database\n",
    "        for video_path in db_videos:\n",
    "            print(f\"    Processing: {os.path.basename(video_path)}\")\n",
    "            try:\n",
    "                # Extract embedding from video\n",
    "                embedding = extract_embedding_from_video(model, processor, video_path, config, device)\n",
    "                if embedding is not None:\n",
    "                    embedding = embedding.flatten()\n",
    "                    X_db.append(embedding)\n",
    "                    y_db.append(label)  # Use label string instead of number\n",
    "                    print(f\"      ‚úÖ Successfully added embedding\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå Failed to extract embedding\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Add to test list\n",
    "        test_tl_videos_for_test.append(test_video)\n",
    "        test_tl_labels_for_test.append(label)\n",
    "    else:\n",
    "        print(f\"[WARNING] Gloss '{label}' has only {len(videos)} videos, skipping...\")\n",
    "\n",
    "# Update database\n",
    "print(f\"\\n[INFO] Updated database:\")\n",
    "print(f\"  - Total samples: {len(X_db)}\")\n",
    "print(f\"  - Unique labels: {len(set(y_db))}\")\n",
    "\n",
    "# Retrain KNN with updated data\n",
    "print(f\"\\n[INFO] Retraining KNN classifier...\")\n",
    "if len(X_db) > 0:\n",
    "    # Convert to numpy for KNN training\n",
    "    if isinstance(X_db[0], np.ndarray):\n",
    "        X_db_array = np.vstack(X_db)\n",
    "    else:\n",
    "        X_db_array = np.array(X_db)\n",
    "    \n",
    "    y_db_array = np.array(y_db)\n",
    "    \n",
    "    # Use appropriate k for KNN\n",
    "    k_neighbors = min(3, len(set(y_db)))\n",
    "    knn_updated = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "    knn_updated.fit(X_db_array, y_db_array)\n",
    "    \n",
    "    print(f\"  ‚úÖ KNN retrained with k={k_neighbors}\")\n",
    "    \n",
    "    # Test on both datasets\n",
    "    print(f\"\\n[INFO] Testing on ALL held-out videos:\")\n",
    "    # 2. Test on data_tl_test videos\n",
    "    print(f\"\\n=== Testing on data_tl_test videos ===\")\n",
    "    correct_tl = 0\n",
    "    total_tl = len(test_tl_videos_for_test)\n",
    "    \n",
    "    for i, (test_video, expected_label) in enumerate(zip(test_tl_videos_for_test, test_tl_labels_for_test)):\n",
    "        print(f\"\\n  Test {i+1}/{total_tl}: {os.path.basename(test_video)}\")\n",
    "        print(f\"    Expected: {expected_label}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract embedding\n",
    "            test_embedding = extract_embedding_from_video(model, processor, test_video, config, device)\n",
    "            if test_embedding is not None:\n",
    "                test_embedding = test_embedding.flatten()\n",
    "                \n",
    "                # Predict\n",
    "                pred_label = knn_updated.predict(test_embedding.reshape(1, -1))[0]\n",
    "                \n",
    "                # Get prediction confidence (distance to nearest neighbors)\n",
    "                distances, indices = knn_updated.kneighbors(test_embedding.reshape(1, -1), n_neighbors=k_neighbors)\n",
    "                avg_distance = distances[0].mean()\n",
    "                \n",
    "                print(f\"    Predicted: {pred_label}\")\n",
    "                print(f\"    Avg distance: {avg_distance:.4f}\")\n",
    "                print(f\"    Result: {'‚úÖ CORRECT' if pred_label == expected_label else '‚ùå WRONG'}\")\n",
    "                \n",
    "                if pred_label == expected_label:\n",
    "                    correct_tl += 1\n",
    "            else:\n",
    "                print(f\"    ‚ùå Failed to extract embedding\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\nüìù New data_tl_test:\")\n",
    "    print(f\"  Correct: {correct_tl}/{total_tl}\")\n",
    "    print(f\"  Accuracy: {correct_tl/total_tl*100:.1f}%\" if total_tl > 0 else \"  No test videos\")\n",
    "    \n",
    "else:\n",
    "    print(f\"  ‚ùå No data in database to train KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"data/datatest\"  # Replace with your video path\n",
    "videos = glob.glob(os.path.join(test_dir, \"*.mp4\"))\n",
    "count = 0\n",
    "for video_path in videos:\n",
    "    filename = os.path.basename(video_path)\n",
    "    number = int(filename.split('_')[1].split('.')[0])\n",
    "    embedding = extract_embedding_from_video(model, processor, video_path, config, device)\n",
    "    pred_label = knn_updated.predict(embedding)\n",
    "    print(f\"{number}: {pred_label}\")\n",
    "    if number == int(pred_label[0]):\n",
    "        count += 1\n",
    "print(f\"{count}/{len(videos)} videos predicted correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vslr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
