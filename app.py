import streamlit as st
import os
import torch
import cv2
import numpy as np
import time
from datetime import datetime

# --- IMPORT C√ÅC TH√ÄNH PH·∫¶N C·∫¶N THI·∫æT ---
from scripts.inference import predict_from_video 
from src.utils.detector import MediaPipeProcessor
from src.utils.model_utils import create_model, create_adjacency_matrix
from src.utils.data_utils import load_labels_from_csv
from configs.config import Config

# --- C√ÅC H√ÄM T·∫¢I D·ªÆ LI·ªÜU V√Ä MODEL ---
@st.cache_resource
def load_model_and_config():
    st.info("üîÑ L·∫ßn ch·∫°y ƒë·∫ßu ti√™n, ƒëang t·∫£i model v√† c·∫•u h√¨nh...")
    config = Config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    _, _, _, id_to_label_mapping = load_labels_from_csv(None, config)
    num_classes = len(id_to_label_mapping)
    A = create_adjacency_matrix(config)
    model = create_model(config, A, num_classes=num_classes, device=device)
    model_path = 'outputs/models/best_hgc_lstm.pth'
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model, config, device, id_to_label_mapping

model, config, device, id_to_label_mapping = load_model_and_config()

# --- GIAO DI·ªÜN ·ª®NG D·ª§NG STREAMLIT ---
st.set_page_config(page_title="VSLR Inference", layout="wide")
st.title("ü§ü Nh·∫≠n di·ªán Ng√¥n ng·ªØ K√Ω hi·ªáu Ti·∫øng Vi·ªát (VSLR)")

# Kh·ªüi t·∫°o session state
if 'camera_active' not in st.session_state:
    st.session_state.camera_active = False
if 'recording' not in st.session_state:
    st.session_state.recording = False
if 'recorded_frames' not in st.session_state:
    st.session_state.recorded_frames = []
if 'recording_start_time' not in st.session_state:
    st.session_state.recording_start_time = 0

# --- SIDEBAR ---
with st.sidebar:
    st.header("‚öôÔ∏è T√πy ch·ªçn")
    option = st.radio("Ch·ªçn ph∆∞∆°ng th·ª©c ƒë·∫ßu v√†o:", ["S·ª≠ d·ª•ng Camera", "T·∫£i l√™n Video"])
    st.markdown("---")
    show_skeleton = st.checkbox("Hi·ªÉn th·ªã khung x∆∞∆°ng", value=True)
    conf_threshold = st.slider("Ng∆∞·ª°ng tin c·∫≠y", min_value=0.0, max_value=1.0, value=0.7, step=0.05)

# --- KHU V·ª∞C HI·ªÇN TH·ªä CH√çNH ---

# T√ôY CH·ªåN 1: T·∫¢I L√äN VIDEO (Kh√¥ng thay ƒë·ªïi)
if option == "T·∫£i l√™n Video":
    # (Gi·ªØ nguy√™n ph·∫ßn code x·ª≠ l√Ω T·∫£i l√™n Video)
    st.header("üìÅ T·∫£i l√™n Video ƒë·ªÉ d·ª± ƒëo√°n")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file video", type=["mp4", "avi", "mov"])
    
    if uploaded_file is not None:
        temp_dir = "data/datatest_streamlit"
        os.makedirs(temp_dir, exist_ok=True)
        temp_video_path = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_video_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        st.video(temp_video_path)
        
        if st.button("B·∫Øt ƒë·∫ßu D·ª± ƒëo√°n Video", use_container_width=True):
            with st.spinner("ƒêang x·ª≠ l√Ω v√† d·ª± ƒëo√°n..."):
                processor = MediaPipeProcessor(config)
                label, _ = predict_from_video(model, processor, id_to_label_mapping, config, device, temp_video_path, thresh_hold=conf_threshold)
                
                if label:
                    st.success(f"üéØ **K·∫øt qu·∫£ d·ª± ƒëo√°n: {label.upper()}**")
                else:
                    st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n (ƒë·ªô tin c·∫≠y th·∫•p).")


# T√ôY CH·ªåN 2: S·ª¨ D·ª§NG CAMERA
elif option == "S·ª≠ d·ª•ng Camera":
    st.header("üìπ D·ª± ƒëo√°n tr·ª±c ti·∫øp qua Camera")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üé• B·∫≠t Camera", use_container_width=True):
            st.session_state.camera_active = True
            st.rerun() # Ch·∫°y l·∫°i script ƒë·ªÉ k√≠ch ho·∫°t camera ngay l·∫≠p t·ª©c
        if st.button("üî¥ B·∫Øt ƒë·∫ßu Ghi", use_container_width=True):
            if st.session_state.camera_active:
                st.session_state.recording = True
                st.session_state.recorded_frames = []
                st.session_state.recording_start_time = time.time()
            else:
                st.error("Vui l√≤ng b·∫≠t camera tr∆∞·ªõc!")

    with col2:
        if st.button("‚èπÔ∏è T·∫Øt Camera", use_container_width=True):
            st.session_state.camera_active = False
            st.session_state.recording = False
            st.session_state.recorded_frames = [] # X√≥a frame khi t·∫Øt cam
            st.rerun()
        if st.button("‚è∏Ô∏è D·ª´ng Ghi & D·ª± ƒëo√°n", use_container_width=True):
            # Ch·ªâ c·∫ßn set recording = False, logic x·ª≠ l√Ω s·∫Ω ƒë∆∞·ª£c k√≠ch ho·∫°t ·ªü d∆∞·ªõi
            if st.session_state.recording:
                st.session_state.recording = False
                st.rerun()

    # --- B·∫ÆT ƒê·∫¶U LOGIC M·ªöI ---
    # KI·ªÇM TRA V√Ä X·ª¨ L√ù VIDEO V·ª™A GHI XONG
    # Logic n√†y ch·∫°y khi kh√¥ng c√≤n ·ªü tr·∫°ng th√°i ghi, nh∆∞ng v·∫´n c√≤n frame ƒë√£ l∆∞u
    if not st.session_state.recording and st.session_state.get('recorded_frames'):
        frames_to_save = st.session_state.recorded_frames
        with st.spinner("ƒêang x·ª≠ l√Ω video v√† d·ª± ƒëo√°n..."):
            save_dir = "data/data_test_cam"
            os.makedirs(save_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_video_path = os.path.join(save_dir, f"test_cam_{timestamp}.mp4")
            
            stop_time = time.time()
            start_time = st.session_state.get('recording_start_time', stop_time)
            elapsed_time = stop_time - start_time
            actual_fps = len(frames_to_save) / elapsed_time if elapsed_time > 0 else 20.0
            
            height, width, _ = frames_to_save[0].shape
            video_writer = cv2.VideoWriter(temp_video_path, cv2.VideoWriter_fourcc(*'mp4v'), actual_fps, (width, height))
            for f in frames_to_save:
                video_writer.write(f)
            video_writer.release()
            st.toast(f"‚úÖ Video t·∫°m ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {temp_video_path}")

            processor = MediaPipeProcessor(config)
            label, _ = predict_from_video(model, processor, id_to_label_mapping, config, device, temp_video_path, thresh_hold=conf_threshold)

            if label:
                st.success(f"üéØ **K·∫øt qu·∫£ d·ª± ƒëo√°n: {label.upper()}**")
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n (ƒë·ªô tin c·∫≠y th·∫•p).")
        
        # D·ªçn d·∫πp state ƒë·ªÉ s·∫µn s√†ng cho l·∫ßn ghi ti·∫øp theo
        st.session_state.recorded_frames = []

    # HI·ªÇN TH·ªä CAMERA N·∫æU ƒêANG K√çCH HO·∫†T
    if st.session_state.camera_active:
        video_placeholder = st.empty()
        cap = cv2.VideoCapture(0)
        processor = MediaPipeProcessor(config)

        while st.session_state.camera_active:
            ret, frame = cap.read()
            if not ret:
                st.error("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera.")
                break
            
            frame = cv2.flip(frame, 1)
            original_frame, res = processor.process_frame(frame)
            display_frame = processor.draw_landmarks(original_frame, res) if show_skeleton else original_frame
            
            if st.session_state.recording:
                st.session_state.recorded_frames.append(frame.copy())
                elapsed_time = time.time() - st.session_state.recording_start_time
                cv2.putText(display_frame, f"RECORDING {elapsed_time:.1f}s", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

            video_placeholder.image(cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB), channels="RGB", use_container_width=True)
        
        cap.release()
    else:
        st.info("Nh·∫•n 'B·∫≠t Camera' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")