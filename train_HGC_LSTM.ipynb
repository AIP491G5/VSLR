{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import project modules from new structure\n",
    "from configs.config import Config\n",
    "from src.utils.data_utils import (\n",
    "    load_labels_from_csv, \n",
    "    SignLanguageDataset, \n",
    "    create_data_loaders,\n",
    "    flip_keypoints,\n",
    "    transform_keypoints\n",
    ")\n",
    "from src.utils.model_utils import (\n",
    "    create_adjacency_matrix,\n",
    "    create_model\n",
    ")\n",
    "from src.models.model import (\n",
    "    HGC_LSTM\n",
    ")\n",
    "from src.utils.train_utils import (\n",
    "    train_model\n",
    ")\n",
    "from src.utils.visualization_utils import (\n",
    "    visualize_training_process,\n",
    "    analyze_model_performance\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"üì¶ Project structure reorganized with modular imports\")\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîß CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "print()\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"üîß Configuration loaded from configs/config.py\")\n",
    "print(f\"üìä Data from: {config.data.input_kp_path}\")\n",
    "print(f\"üíæ Models save to: {config.model.checkpoint_dir}\")\n",
    "print(f\"üìà Plots save to: {config.output.plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data utilities\n",
    "from src.utils.data_utils import load_labels_from_csv\n",
    "\n",
    "# Load labels\n",
    "video_to_label_mapping, label_to_idx, unique_labels, id_to_label_mapping = load_labels_from_csv(None, config)\n",
    "num_classes = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data augmentation and dataset utilities\n",
    "from src.utils.data_utils import flip_keypoints, transform_keypoints, SignLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with stratified split and augmentation\n",
    "print(\"[INFO] Creating datasets...\")\n",
    "keypoints_dir = config.data.keypoints_output_dir\n",
    "\n",
    "# Set random seed for reproducible splits\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get parameters from config\n",
    "use_strategy = config.data.use_strategy\n",
    "\n",
    "\n",
    "# Training uses augmentation, validation does not (for fair evaluation)\n",
    "train_augmentations = getattr(config.data, 'augmentations', [])\n",
    "val_augmentations = [] \n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"   Split strategy: {'Stratified' if use_strategy else 'Random'}\")\n",
    "print(f\"   Train augmentations: {train_augmentations if train_augmentations else 'None'}\")\n",
    "print(f\"   Val augmentations: {val_augmentations if val_augmentations else 'None (for fair evaluation)'}\")\n",
    "if 'translation' in train_augmentations:\n",
    "    print(f\"   Translation range: ¬±{config.data.translation_range}\")\n",
    "if 'scaling' in train_augmentations:\n",
    "    print(f\"   Scale range: ¬±{config.data.scale_range}\")\n",
    "\n",
    "train_dataset = SignLanguageDataset(\n",
    "    keypoints_dir, video_to_label_mapping, label_to_idx, config,\n",
    "    split_type='train', \n",
    "    augmentations=train_augmentations,\n",
    "    use_strategy=use_strategy\n",
    ")\n",
    "\n",
    "val_dataset = SignLanguageDataset(\n",
    "    keypoints_dir, video_to_label_mapping, label_to_idx, config,\n",
    "    split_type='val', \n",
    "    augmentations=val_augmentations,\n",
    "    use_strategy=use_strategy\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Dataset summary:\")\n",
    "print(f\"  Total classes: {len(unique_labels)}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Strategy: {'Stratified' if use_strategy else 'Random'} split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac722204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data loader utility\n",
    "from src.utils.data_utils import create_data_loaders\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\n[INFO] Creating data loaders...\")\n",
    "train_loader, val_loader = create_data_loaders(train_dataset, val_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ad2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model utilities\n",
    "from src.utils.model_utils import create_adjacency_matrix\n",
    "\n",
    "A = create_adjacency_matrix(config)\n",
    "print(f\"[INFO] Adjacency matrix shape: {A.shape}\")\n",
    "print(f\"[INFO] Number of vertices: {config.hgc_lstm.num_vertices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model classes and utilities\n",
    "from src.utils.model_utils import create_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(config, A, num_classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d719eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Starting training...\")\n",
    "print(f\"[INFO] Training configuration:\")\n",
    "print(f\"  - Epochs: {config.training.num_epochs}\")\n",
    "print(f\"  - Batch size: {config.training.batch_size}\")\n",
    "print(f\"  - Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  - Optimizer: {config.training.optimizer}\")\n",
    "print(f\"  - Scheduler: {config.training.scheduler}\")\n",
    "print(f\"  - Early stopping patience: {config.training.early_stopping_patience}\")\n",
    "history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc62d2",
   "metadata": {},
   "source": [
    "# Training Process Visualization\n",
    "\n",
    "This section provides essential visualization of the training process including:\n",
    "- **Loss curves**: Training and validation loss over epochs\n",
    "- **Accuracy curves**: Training and validation accuracy over epochs  \n",
    "- **Confusion matrix**: Model performance analysis on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae34800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization utilities\n",
    "from src.utils.visualization_utils import visualize_training_process\n",
    "\n",
    "# Visualize training process\n",
    "print(\"[INFO] Generating training process visualization...\")\n",
    "training_fig = visualize_training_process(history, config, save_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model performance analysis\n",
    "from src.utils.visualization_utils import analyze_model_performance\n",
    "from src.utils.model_utils import load_model\n",
    "model_save_path = os.path.join(config.model.checkpoint_dir, config.model.save_name)\n",
    "# model_save_path = os.path.join(config.model.checkpoint_dir, \"best_hgc_lstm_321415.pth\")\n",
    "model = load_model(model_save_path, config)\n",
    "# Analyze model performance\n",
    "print(\"[INFO] Analyzing model performance...\")\n",
    "performance_results = analyze_model_performance(model, val_loader, device, config, unique_labels, id_to_label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39675ec4",
   "metadata": {},
   "source": [
    "# Dual Attention Weights Visualization\n",
    "\n",
    "This section visualizes the **dual attention mechanism** from the HGC-LSTM model:\n",
    "\n",
    "- **Joint Attention**: Shows which keypoints (spatial) the model focuses on after GCN layers\n",
    "- **Temporal Attention**: Shows which time steps the model focuses on after LSTM layers\n",
    "\n",
    "The model architecture: **GCN ‚Üí GCN ‚Üí Joint Attention ‚Üí LSTM ‚Üí Temporal Attention ‚Üí Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da415182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new dual attention visualization functions\n",
    "from src.utils.visualization_utils import visualize_dual_attention_weights, visualize_attention_heatmap\n",
    "\n",
    "# Visualize dual attention weights (joint + temporal)\n",
    "print(\"[INFO] Visualizing dual attention weights...\")\n",
    "print(\"This shows both:\")\n",
    "print(\"  - Joint Attention: Which keypoints are important (after GCN)\")\n",
    "print(\"  - Temporal Attention: Which time steps are important (after LSTM)\")\n",
    "\n",
    "dual_attention_fig = visualize_dual_attention_weights(model, val_loader, device, config, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize joint attention as detailed heatmap\n",
    "print(\"[INFO] Visualizing joint attention heatmap...\")\n",
    "print(\"This shows the actual attention weights from the joint attention layer\")\n",
    "print(\"(Time steps √ó Keypoints) - directly from model's attention mechanism\")\n",
    "\n",
    "joint_attention_heatmap_fig = visualize_attention_heatmap(model, val_loader, device, config, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f50b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.inference import predict_from_video\n",
    "import glob\n",
    "from src.utils.detector import MediaPipeProcessor\n",
    "test_dir = \"data/datatest\"  # Replace with your video path\n",
    "videos = glob.glob(os.path.join(test_dir, \"*.mp4\"))\n",
    "count = 0\n",
    "config = Config()\n",
    "processor = MediaPipeProcessor(config)\n",
    "for video_path in videos:\n",
    "    filename = os.path.basename(video_path)\n",
    "    number = int(filename.split('_')[1].split('.')[0])\n",
    "    label, res = predict_from_video(model, processor, id_to_label_mapping, config, device, test_dir+'/'+filename, thresh_hold=0.2)\n",
    "    print(f\"{number}: {res}\")\n",
    "    if number == res:\n",
    "        count += 1\n",
    "print(f\"{count}/{len(videos)} videos predicted correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vslr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
