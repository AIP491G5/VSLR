# VSLR (Vietnamese Sign Language Recognition)

Há»‡ thá»‘ng nháº­n dáº¡ng ngÃ´n ngá»¯ kÃ½ hiá»‡u Viá»‡t Nam sá»­ dá»¥ng HGC-LSTM(Hierarchical Graph Convolution + LSTM) vá»›i MediaPipe Ä‘á»ƒ trÃ­ch xuáº¥t keypoints tá»« video.

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n nÃ y sá»­ dá»¥ng:

- **MediaPipe** Ä‘á»ƒ trÃ­ch xuáº¥t keypoints tá»« video (pose, hands)
- **HGC-LSTM** model Ä‘á»ƒ nháº­n dáº¡ng cá»­ chá»‰
- **Graph Convolution** Ä‘á»ƒ model má»‘i quan há»‡ giá»¯a cÃ¡c keypoints
- **LSTM** Ä‘á»ƒ model temporal dynamics
- **Data Augmentation** Ä‘á»ƒ tÄƒng cÆ°á»ng dá»¯ liá»‡u (future)

## ğŸš€ Quy trÃ¬nh thá»±c hiá»‡n

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u video vÃ  táº¡o CSV labels

```bash
python extract_csv.py
```

**Má»¥c Ä‘Ã­ch:** Tá»• chá»©c video tá»« `new_dataset/` theo format chuáº©n vÃ  táº¡o file `labels.csv`

**Input:**

- Folder: `new_dataset/` chá»©a video vá»›i format `<id>_<label>_<other>.mp4`
- VÃ­ dá»¥: `1_xin_chao_001.mp4`, `1_xin_chao_002.mp4`, `2_cam_on_001.mp4`

**Output:**

- Folder: `new_data/` chá»©a video Ä‘Ã£ Ä‘á»•i tÃªn: `1_01.mp4`, `1_02.mp4`, `2_01.mp4`, etc.
- File: `labels.csv` chá»©a mapping giá»¯a ID vÃ  label

```csv
id,label,videos
1,xin_chao,"1_01.mp4, 1_02.mp4, 1_03.mp4"
2,cam_on,"2_01.mp4, 2_02.mp4, 2_03.mp4"
```

### BÆ°á»›c 2: TrÃ­ch xuáº¥t keypoints

```bash
python extract_kpts_n_label.py
```

**Má»¥c Ä‘Ã­ch:** Sá»­ dá»¥ng MediaPipe Ä‘á»ƒ trÃ­ch xuáº¥t keypoints tá»« video

**Input:** Video tá»« `data/Videos/`
**Output:**

- `data/Keypoints/` chá»©a keypoints files (`.npy`)
- `data/Labels/` chá»©a label files (`.npy`)

**Keypoints format:**

- Shape: `(T, 150)` where T = sá»‘ frames, 150 = 75 keypoints Ã— 2 coordinates
- 75 keypoints: 33 pose + 21 left hand + 21 right hand

### BÆ°á»›c 3: Training Model

```bash
jupyter notebook train_HGC_LSTM.ipynb
```

hoáº·c cháº¡y tá»«ng cell trong notebook.

## ğŸ”§ Cáº¥u hÃ¬nh

Táº¥t cáº£ parameters Ä‘Æ°á»£c quáº£n lÃ½ trong `config.py`:

### Data Configuration

```python
@dataclass
class DataConfig:
    input_csv_file: str = "labels.csv"
    video_input_dir: str = "new_data"
    keypoints_output_dir: str = "data/Keypoints"
    sequence_length: int = 60
    train_split: float = 0.9
```

###HGC-LSTMConfiguration

```python
@dataclass
class HGCLSTMConfig:
    sequence_length: int = 60
    num_vertices: int = 75
    in_channels: int = 2
    hidden_gcn: int = 128
    hidden_lstm: int = 128
    dropout: float = 0.5
    pooling_type: str = "attention"

    # Data Augmentation
    data_augmentation: bool = True
    scale_factors: list = [0.9, 1.0, 1.1]
    translation_x: list = [0.1, 0.15, 0.2, -0.1, -0.15, -0.2]
```

### Training Configuration

```python
@dataclass
class TrainingConfig:
    num_epochs: int = 100
    batch_size: int = 8
    learning_rate: float = 1e-3
    optimizer: str = "adam"
    scheduler: str = "step"
    early_stopping_patience: int = 50
    model_save_name: str = "best_hgc_lstm.pth"
```

## ğŸ“Š Data Augmentation

Há»‡ thá»‘ng há»— trá»£ data augmentation Ä‘á»ƒ tÄƒng cÆ°á»ng dataset:

**Má»—i sample gá»‘c sáº½ táº¡o ra 18 augmented samples:**

- 3 scale factors (0.9, 1.0, 1.1)
- 6 translation values (Â±0.1, Â±0.15, Â±0.2)
- Total: 3 Ã— 6 = 18 combinations

**CÃ¡ch báº­t/táº¯t:**

```python
# Trong notebook
USE_DATA_AUGMENTATION = True  # hoáº·c False

# Hoáº·c trong config.py
data_augmentation: bool = True
```

## ğŸ—ï¸ Model Architecture

### HGC-LSTMModel

```
Input: (B, T, V, C) = (batch, 60, 75, 2)
â†“
GCN Layer 1: (B, T, V, 128)
â†“
GCN Layer 2: (B, T, V, 128)
â†“
Spatial Pooling: (B, T, 128)
â†“
LSTM: (B, T, 128)
â†“
Take last timestep: (B, 128)
â†“
Dropout + FC: (B, num_classes)
```

### Graph Convolution

- **Adjacency Matrix:** 75Ã—75 modeling skeleton connections
- **Pose connections:** MediaPipe pose landmarks
- **Hand connections:** Left hand (33-53) + Right hand (54-74)
- **Normalization:** Symmetric normalized adjacency matrix

### Spatial Pooling Types

- **adaptive_avg:** Average pooling over joints
- **adaptive_max:** Max pooling over joints
- **attention:** Learnable attention weights

## ğŸ“ˆ Training Process

### 1. Data Loading

```python
# Tá»± Ä‘á»™ng load tá»« config
train_dataset = SignLanguageDataset(split_type='train')
val_dataset = SignLanguageDataset(split_type='val')
```

### 2. Model Training

```python
# Vá»›i data augmentation: 18x samples
# Vá»›i early stopping vÃ  learning rate scheduling
history = train_model(model, train_loader, val_loader, config, device)
```

### 3. Evaluation

```python
# Load best model vÃ  evaluate
model.load_state_dict(torch.load('best_hgc_lstm.pth'))
accuracy = evaluate_model(model, val_loader, device)
```

## ğŸ¯ Inference

```python
# Predict single sequence
result = predict_sequence(model, keypoints, device, labels)
print(f"Predicted: {result['predicted_label']}")
print(f"Confidence: {result['confidence']:.4f}")
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
VSLR/
â”œâ”€â”€ config.py                 # Configuration management
â”œâ”€â”€ extract_csv.py            # Video organization & CSV creation
â”œâ”€â”€ cv_to_60.py              # Video FPS conversion
â”œâ”€â”€ extract_kpts_n_label.py  # Keypoints extraction
â”œâ”€â”€ train_HGC_LSTM.ipynb    # Training notebook
â”œâ”€â”€ detector.py              # MediaPipe processing
â”œâ”€â”€ labels.csv               # Labels mapping
â”œâ”€â”€ new_dataset/             # Original videos
â”œâ”€â”€ new_data/                # Organized videos
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Videos/              # 60 FPS videos
â”‚   â”œâ”€â”€ Keypoints/           # Extracted keypoints (.npy)
â”‚   â””â”€â”€ Labels/              # Labels (.npy)
â””â”€â”€ models/
    â””â”€â”€ best_hgc_lstm.pth   # Trained model
```

## ğŸ” Troubleshooting

### Common Issues

1. **CUDA Out of Memory:**

   - Giáº£m `batch_size` trong config
   - Táº¯t data augmentation: `data_augmentation = False`

2. **Low Accuracy:**

   - TÄƒng `num_epochs`
   - Thá»­ `pooling_type = "attention"`
   - Kiá»ƒm tra data quality

3. **Training quÃ¡ cháº­m:**
   - Giáº£m `sequence_length`
   - Giáº£m `hidden_gcn` vÃ  `hidden_lstm`
   - Táº¯t data augmentation

### Performance Tips

1. **Tá»‘i Æ°u Data Augmentation:**

```python
# Giáº£m augmentation combinations
scale_factors = [0.9, 1.0, 1.1]     # 3 scales
translation_x = [0.1, -0.1]         # 2 translations
# Total: 3 Ã— 2 = 6 combinations thay vÃ¬ 18
```

2. **Tá»‘i Æ°u Model:**

```python
# Giáº£m model complexity
hidden_gcn = 64
hidden_lstm = 64
dropout = 0.3
```

3. **Tá»‘i Æ°u Training:**

```python
# Faster convergence
learning_rate = 2e-3
scheduler_step_size = 10
scheduler_gamma = 0.7
```

## ğŸŠ Káº¿t quáº£

Model sáº½ output:

- **Training history:** Loss vÃ  accuracy curves
- **Best model:** Saved as `best_hgc_lstm.pth`
- **Classification report:** Precision, recall, F1-score cho tá»«ng class

---

**NgÃ y cáº­p nháº­t:** July 2025
